{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "from hyperparams import Hyperparams as hp\n",
    "from data_load import get_batch_data, load_de_vocab, load_en_vocab\n",
    "from modules import *\n",
    "import os, codecs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 124)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m124\u001b[0m\n\u001b[0;31m    self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=len(en2idx)))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Graph():\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if is_training:\n",
    "                self.x, self.y, self.num_batch = get_batch_data() # (N, T)\n",
    "            else: # inference\n",
    "                self.x = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "                self.y = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "\n",
    "            # define decoder inputs\n",
    "            self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 2:<S>\n",
    "\n",
    "            # Load vocabulary    \n",
    "            de2idx, idx2de = load_de_vocab()\n",
    "            en2idx, idx2en = load_en_vocab()\n",
    "                        # Encoder\n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                ## Embedding\n",
    "                self.enc = embedding(self.x, \n",
    "                                      vocab_size=len(de2idx), \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      scale=True,\n",
    "                                      scope=\"enc_embed\")\n",
    "                \n",
    "                ## Positional Encoding\n",
    "                if hp.sinusoid:\n",
    "                    self.enc += positional_encoding(self.x,\n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"enc_pe\")\n",
    "                else:\n",
    "                    self.enc += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"enc_pe\")\n",
    "                    \n",
    "                 \n",
    "                ## Dropout\n",
    "                self.enc = tf.layers.dropout(self.enc, \n",
    "                                            rate=hp.dropout_rate, \n",
    "                                            training=tf.convert_to_tensor(is_training))\n",
    "                \n",
    "                ## Blocks\n",
    "                for i in range(hp.num_blocks):\n",
    "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                        ### Multihead Attention\n",
    "                        self.enc = multihead_attention(queries=self.enc, \n",
    "                                                        keys=self.enc, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads, \n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training,\n",
    "                                                        causality=False)\n",
    "                        \n",
    "                        ### Feed Forward\n",
    "                        self.enc = feedforward(self.enc, num_units=[4*hp.hidden_units, hp.hidden_units])\n",
    "                                    # Decoder\n",
    "            with tf.variable_scope(\"decoder\"):\n",
    "                ## Embedding\n",
    "                self.dec = embedding(self.decoder_inputs, \n",
    "                                      vocab_size=len(en2idx), \n",
    "                                      num_units=hp.hidden_units,\n",
    "                                      scale=True, \n",
    "                                      scope=\"dec_embed\")\n",
    "                \n",
    "                ## Positional Encoding\n",
    "                if hp.sinusoid:\n",
    "                    self.dec += positional_encoding(self.decoder_inputs,\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"dec_pe\")\n",
    "                else:\n",
    "                    self.dec += embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.decoder_inputs)[1]), 0), [tf.shape(self.decoder_inputs)[0], 1]),\n",
    "                                      vocab_size=hp.maxlen, \n",
    "                                      num_units=hp.hidden_units, \n",
    "                                      zero_pad=False, \n",
    "                                      scale=False,\n",
    "                                      scope=\"dec_pe\")\n",
    "                \n",
    "                ## Dropout\n",
    "                self.dec = tf.layers.dropout(self.dec, \n",
    "                                            rate=hp.dropout_rate, \n",
    "                                            training=tf.convert_to_tensor(is_training))\n",
    "                                ## Blocks\n",
    "                for i in range(hp.num_blocks):\n",
    "                    with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "                        ## Multihead Attention ( self-attention)\n",
    "                        self.dec = multihead_attention(queries=self.dec, \n",
    "                                                        keys=self.dec, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads, \n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training,\n",
    "                                                        causality=True, \n",
    "                                                        scope=\"self_attention\")\n",
    "                        \n",
    "                        ## Multihead Attention ( vanilla attention)\n",
    "                        self.dec = multihead_attention(queries=self.dec, \n",
    "                                                        keys=self.enc, \n",
    "                                                        num_units=hp.hidden_units, \n",
    "                                                        num_heads=hp.num_heads,\n",
    "                                                        dropout_rate=hp.dropout_rate,\n",
    "                                                        is_training=is_training, \n",
    "                                                        causality=False,\n",
    "                                                        scope=\"vanilla_attention\")\n",
    "                        \n",
    "                        ## Feed Forward\n",
    "                        self.dec = feedforward(self.dec, num_units=[4*hp.hidden_units, hp.hidden_units])\n",
    "                \n",
    "            # Final linear projection\n",
    "            self.logits = tf.layers.dense(self.dec, len(en2idx))\n",
    "            self.preds = tf.to_int32(tf.arg_max(self.logits, dimension=-1))\n",
    "            self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "            self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
    "            tf.summary.scalar('acc', self.acc)\n",
    "                        if is_training:  \n",
    "                # Loss\n",
    "                self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=len(en2idx)))\n",
    "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
    "                self.mean_loss = tf.reduce_sum(self.loss*self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "               \n",
    "                # Training Scheme\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "                self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "                   \n",
    "                # Summary \n",
    "                tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "                self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dfdb3099d470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Construct graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Graph loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Start session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Graph' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':                \n",
    "    # Load vocabulary    \n",
    "    de2idx, idx2de = load_de_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "    \n",
    "    # Construct graph\n",
    "    g = Graph(\"train\"); print(\"Graph loaded\")\n",
    "    \n",
    "    # Start session\n",
    "    sv = tf.train.Supervisor(graph=g.graph, \n",
    "                             logdir=hp.logdir,\n",
    "                             save_model_secs=0)\n",
    "    with sv.managed_session() as sess:\n",
    "        for epoch in range(1, hp.num_epochs+1): \n",
    "            if sv.should_stop(): break\n",
    "            for step in tqdm(range(g.num_batch), total=g.num_batch, ncols=70, leave=False, unit='b'):\n",
    "                sess.run(g.train_op)\n",
    "                \n",
    "            gs = sess.run(g.global_step)   \n",
    "            sv.saver.save(sess, hp.logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))\n",
    "    \n",
    "    print(\"Done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
